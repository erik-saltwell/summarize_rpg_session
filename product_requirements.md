Great, I’ll draft a detailed product requirements document for the `summarize_rpg_session` command-line application. It will outline functionality such as audio transcription with speaker diarization, detailed session summarization using OpenAI’s GPT-4o, markdown output, and progress tracking via console.

I’ll let you know as soon as it's ready for review.

# **summarize_rpg_session** – Product Requirements Document

## Overview  
**summarize_rpg_session** is a Unix-based command-line tool that automates the transcription and summarization of tabletop roleplaying game (RPG) sessions. It allows Game Masters and players to turn long session recordings or transcripts into clear, detailed session notes. The goal is to capture every plot point, character action, and decision from a gaming session without manual note-taking. For instance, using AI to process dozens of hours of RPG session recordings can yield a knowledge base of the campaign’s events, letting a GM query what happened in each session or who did what ([Using Whisper and NotebookLM to Build a Private ChatGPT For Your Campaign — Advanced Old School Revival — OSR+](https://osrplus.com/gm-corner/using-whisper-and-notebooklm-to-build-a-private-chatgpt-for-your-campaign/#:~:text=In%20this%20post%2C%20I%27ll%20show,recall%20exactly%20what%20was%20said)). By leveraging OpenAI’s transcription and language models, this tool produces a **diarized** transcript (with speakers labeled) and a comprehensive markdown summary that can be easily read or shared. This saves users time – imagine turning a two-hour recording into a concise write-up, making key points far easier to review and recall ([Creating an Audio Transcription and Summarization with OpenAI’s Whisper and Python | by Alex Rodrigues | Medium](https://medium.com/@alexrodriguesj/creating-an-audio-transcription-and-summarization-with-openais-whisper-and-python-860b41dfac8c#:~:text=When%20dealing%20with%20lengthy%20audio,key%20points%20easier%20to%20consume)).  

## Features  
- **Flexible Input Options:** Accepts either an audio recording or an existing text transcript of an RPG session. The user can provide a path or URL to an **audio file** (supported formats: `.wav` or `.mp3`), or a path to a pre-written **text transcript** of the session. This flexibility accommodates both those who record sessions and those who have transcripts prepared. The tool will validate that one (and only one) of these input types is provided, preventing ambiguity or missing input.  

- **Audio Transcription with Speaker Diarization:** If an audio file is given, the application uses OpenAI’s transcription API (e.g. a Whisper/GPT-4 based service) to convert speech to text. The raw transcript is then **diarized** – meaning it is segmented and labeled by speaker turns. This diarization annotates who said what throughout the session, using provided character or player names if available, or generic labels (“Speaker 1”, “Speaker 2”, etc.) by default. In speech recognition, diarization is the process of partitioning an audio stream into segments corresponding to different speakers ([Speaker Labels and Speaker Diarization Explained: How to Obtain and Use Them for Accurate Transcription](https://www.recall.ai/post/speaker-labels-and-speaker-diarization-explained-how-to-obtain-and-use-them-for-accurate-transcription#:~:text=However%2C%20for%20LLMs%20to%20produce,transcript%20must%20contain%20speaker%20labels)), effectively adding speaker labels that tell you who spoke each word of the transcript ([Speaker Labels and Speaker Diarization Explained: How to Obtain and Use Them for Accurate Transcription](https://www.recall.ai/post/speaker-labels-and-speaker-diarization-explained-how-to-obtain-and-use-them-for-accurate-transcription#:~:text=However%2C%20for%20LLMs%20to%20produce,transcript%20must%20contain%20speaker%20labels)). Incorporating speaker labels is critical for accuracy, as Large Language Models (LLMs) can analyze transcripts more effectively when they know who is speaking ([Speaker Labels and Speaker Diarization Explained: How to Obtain and Use Them for Accurate Transcription](https://www.recall.ai/post/speaker-labels-and-speaker-diarization-explained-how-to-obtain-and-use-them-for-accurate-transcription#:~:text=However%2C%20for%20LLMs%20to%20produce,transcript%20must%20contain%20speaker%20labels)). (For example, identifying which character made a decision or statement helps the summary correctly assign actions to the right person.) The diarization should handle various numbers of speakers and use heuristics or AI to distinguish them. If the user supplies known speaker names (e.g. via a config or arguments), the app will map the generic labels to these names for a more readable transcript.  

- **Detailed Session Summarization:** The core output is a **highly detailed summary** of the RPG session, generated by an OpenAI LLM (such as GPT-4). The application feeds the full session transcript to the LLM and prompts it to produce a thorough narrative recap of everything that transpired. The summary will be written in a clear, **markdown-formatted** document, suitable for session logs or wikis. It will capture all important plot developments, **character actions**, significant decisions, and outcomes from the session. The tone of the summary should remain factual and detailed, essentially retelling the session. It will include direct quotes from the players or game master only if those lines are particularly memorable or pivotal to the story (to avoid clutter, routine dialogue is paraphrased rather than quoted). The output should serve as a faithful record of the session, allowing readers (even those who didn’t attend) to understand what happened and what plot threads are ongoing.  

- **Progress Feedback and User Experience:** The tool provides real-time console feedback to keep the user informed of progress. As it runs, it will display clear status messages or a progress bar indicating each major step (transcribing audio, diarizing transcript, summarizing text, etc.). This is achieved using a well-supported Python library for rich console output (such as `tqdm` or `rich`). Providing a progress indicator reassures the user that the lengthy transcription and analysis tasks are underway and prevents the impression that the tool has stalled ([Python | How to make a terminal progress bar using tqdm - GeeksforGeeks](https://www.geeksforgeeks.org/python-how-to-make-a-terminal-progress-bar-using-tqdm/#:~:text=Whether%20you%E2%80%99re%20installing%20software%2C%20loading,and%20make%20it%20look%20lively)). **For example:** the CLI might show a progress bar with a percentage and elapsed time during transcription and summarization.  ([Python | How to make a terminal progress bar using tqdm - GeeksforGeeks](https://www.geeksforgeeks.org/python-how-to-make-a-terminal-progress-bar-using-tqdm/)) This screenshot shows an example of a console progress bar (with a descriptive label and percentage complete) updating as the application processes a task, giving immediate visual feedback to the user ([Python | How to make a terminal progress bar using tqdm - GeeksforGeeks](https://www.geeksforgeeks.org/python-how-to-make-a-terminal-progress-bar-using-tqdm/#:~:text=Whether%20you%E2%80%99re%20installing%20software%2C%20loading,and%20make%20it%20look%20lively)). Each stage of the process will either update the progress bar or print a message (e.g. “Transcribing audio…”, “Generating summary…”), so the experience remains user-friendly even for long sessions.  

- **Unix-Based Platform:** **summarize_rpg_session** is designed for Unix-like operating systems (Linux, macOS, etc.) and runs as a one-shot command-line program. It does not require a persistent server or GUI – users invoke it from the terminal when they need to summarize a session. The tool will follow Unix conventions for file paths and execution. (While not officially supported on Windows in this version, it should run in a Windows environment that provides a Unix-like shell or Python environment, though path handling is primarily geared toward Unix-style paths.) No installation beyond the necessary Python environment and dependencies is required; the application can be distributed as a Python package or standalone script for Unix systems.  

- **Stateless Execution and Robust Error Handling:** The application is stateless – each run is independent and there is no persistent configuration or memory of past runs. All necessary parameters (input file paths, output paths, etc.) must be provided each time. This one-shot design keeps usage simple and avoids any hidden state between runs. The tool will validate all inputs on startup and provide **clear error messages** if something is amiss. For example, if the user forgets to specify an output path, or provides an audio file path that does not exist, the tool will immediately print a friendly error explaining the issue (e.g. “Error: Audio file not found at the given path.”). It will similarly handle unsupported file formats or other misuse with informative messages. During processing, any runtime errors (such as inability to reach the OpenAI API) will be caught, and the tool will output a descriptive error rather than crashing or hanging. Rate limit issues with the OpenAI API are handled gracefully by the tool (it will back off and retry requests rather than aborting on the first failure). In summary, the user experience is designed to be robust: either a successful transcript and summary, or a clear explanation of what went wrong and how to fix it.

## Inputs and Outputs  

**Input Parameters:** The application accepts the following inputs from the user (via command-line arguments): 

- **Audio File** (optional): Path or URL to an audio recording of the RPG session in `.wav` or `.mp3` format. If an audio file is provided, the tool will perform transcription. (Example usage: `summarize_rpg_session --audio session1.mp3 --output summary1.md`.)  
- **Text Transcript** (optional): Path to an existing text transcript file of the session (e.g. a `.txt` file containing dialogue or notes). If a transcript is provided, transcription is skipped and the given text will be used for summarization. This is mutually exclusive with the audio file input – the user must provide either an audio file or a transcript file, not both.  
- **Summary Output Path** (required): Path for the **markdown summary output** file. The user must specify where the generated session summary should be saved (e.g. a `.md` file). If the file exists, it will be overwritten with the new summary (the tool could also warn or prompt before overwriting, but by default it assumes the user has chosen an appropriate output path). The summary will be formatted in Markdown, with suitable headings, lists, and text for easy reading.  
- **Diarized Transcript Output Path** (optional): Path to save the **diarized transcript** text. This is only used when an audio file is provided (since if the user already has a transcript, they presumably don’t need another copy). If specified, after transcribing and diarizing the audio, the tool will write the complete labeled transcript to this path (e.g. as a `.txt` or `.md` file). This transcript will include speaker labels for each segment of speech (for example: “**Alice:** I check the door for traps.” / “**DM:** You don’t see any traps on the door.” on separate lines). If this option is not provided, the tool will still perform diarization internally for the summary’s sake, but it will not output the full transcript to disk.  

All file path inputs are validated: the tool checks that audio or transcript files exist and are accessible, and that the output locations are writable (e.g. it will report an error if it cannot create the output file or if the directory doesn’t exist). In case of an HTTP(S) URL for the audio, the tool will attempt to download the file first or stream it for transcription, handling errors like network failures or invalid URLs with an appropriate message.  

**Outputs:** 

- **Markdown Summary File:** The primary output is a markdown file containing the detailed session summary. This file will be created at the user-specified path (or overwritten if it exists). The summary is structured with markdown syntax: it may include headings for different segments or scenes of the session, bullet points for lists of events or decisions, and bold/italic text if needed for emphasis (for example, to highlight NPC names or important items). The writing is in complete sentences and organized logically (not just bullet points of events, but a coherent narrative of the session). This output file allows the GM or players to easily read what happened or even post the summary to online forums or campaign logs with minimal editing.  
- **Diarized Transcript File:** If the user requested it, a text file containing the full session transcript with speaker labels is output. In this transcript, each line or paragraph is prefixed with the speaker’s name or identifier. For example:  
  ```
  Speaker 1: Alright, I open the treasure chest.  
  Speaker 2 (DM): Inside, you find a glittering sword.  
  Speaker 3: I want to examine the sword’s runes.  
  ```  
  If actual names are known (say the players are Alice, Bob, and a Dungeon Master), those names will be used instead of generic labels for better clarity. The transcript is formatted in a simple, readable text form (could be Markdown with **bold** names or just plain text names). This file is useful for record-keeping or if the user wants to manually review or search the exact dialogue.  

The combination of these outputs gives the user both a **high-level summary** and the **word-for-word record**, catering to different needs. The summary distills the session into a narrative, while the transcript provides transparency and the ability to verify details or extract quotes. Both outputs are stored in the specified files so that the user can access them even after the program finishes (the tool will print a confirmation like “Summary written to **summary1.md**” and “Transcript written to **session1_diarized.txt**” upon completion).

## System Behavior  

When the user runs **summarize_rpg_session** with the appropriate arguments, the application will execute a sequence of steps to produce the desired outputs. The behavior differs slightly depending on whether the input is an audio file or a text transcript, but in both cases the end result is a markdown summary. The following outlines the typical flow:

### 1. Initialization and Validation  
Upon starting, the tool parses the command-line arguments to determine the input and output paths. It verifies that the arguments are valid (exactly one of audio or transcript is provided, an output path for the summary is provided, etc.). If any required argument is missing or invalid, it immediately prints an error and usage instructions, then exits with a non-zero status (so users can correct their command). For example, if both an audio and transcript are given, it will complain that the input is ambiguous; if neither is given, it will prompt that one is required. Similarly, it checks file paths: if the audio file isn’t found or is an unsupported format, the tool notifies the user; if the transcript file can’t be read, it errors out. This up-front validation ensures that the user is informed of any issues before any heavy processing begins.

### 2. Audio Transcription (if audio input)  
If an audio file was provided, the next step is to transcribe the audio into text. The application will call OpenAI’s `gpt-4o-transcribe` API (or a similar high-quality speech-to-text service, likely powered by the Whisper model or equivalent) with the audio data. This may involve uploading the audio file to the API and awaiting the response. The tool shows a progress indicator during this process (for example, a message “Transcribing audio…” with a spinning icon or progress bar). Large audio files might take a while to process, so the user can watch the progress bar move or a percentage complete if available. The app should also handle intermediate feedback if possible (some APIs stream partial transcripts – if supported, the tool could display or log partial results, but primarily it waits for the final transcript).  

OpenAI’s transcription API will return the recognized text of everything said in the recording. The tool collects this raw transcript (which at this point is just a block of text without speaker attribution). It monitors for any API errors – if a network error or API error occurs, the tool will catch it. In case of rate limiting (HTTP 429 errors from the API), the tool will pause and retry after a brief back-off interval rather than fail immediately. (As a guideline, it could implement an exponential backoff strategy as recommended by OpenAI, e.g. waiting a few seconds and retrying, to gracefully overcome transient rate limits ([Rate limits - OpenAI API](https://platform.openai.com/docs/guides/rate-limits#:~:text=One%20easy%20way%20to%20avoid,exponential%20backoff%20means%20performing)).) If after several retries it cannot get a successful transcription due to persistent errors, it will abort and inform the user (e.g. “Transcription failed due to API rate limit. Please try again later.”). Assuming transcription succeeds, the output is a plain text transcript of the entire session.  

### 3. Speaker Diarization (if audio input)  
After obtaining the raw transcript from the audio, the application performs **speaker diarization** on the text. The goal here is to identify segments of the conversation and label them by speaker. Without native speaker IDs from the transcription step (OpenAI’s Whisper API does not inherently label speakers in the single-channel audio), the tool must infer speaker changes. It does this by analyzing pauses or language patterns, or by leveraging an external model or heuristic. If an additional diarization model (such as PyAnnote audio or another ML model) is integrated, the tool will use it to assign speaker labels to chunks of text. Alternatively, an approach using the LLM itself can be taken: for example, feeding the transcript into GPT-4 with a prompt to separate speakers can yield good results, as GPT-4 can often guess speaker shifts based on dialogue style ([Best solution for Whisper diarization/speaker labeling? - API - OpenAI Developer Community](https://community.openai.com/t/best-solution-for-whisper-diarization-speaker-labeling/505922#:~:text=However%2C%20what%20I%20have%20noticed,with%20a%20prompt%20like%20this)). This approach works even better if the user provides the actual speaker names as hints ([Best solution for Whisper diarization/speaker labeling? - API - OpenAI Developer Community](https://community.openai.com/t/best-solution-for-whisper-diarization-speaker-labeling/505922#:~:text=It%20works%20even%20better%20if,to%20get%20a%20foothold%20on)) (e.g., telling GPT-4 the list of participants and maybe an example of each speaking, so it can match lines to names). The chosen implementation will aim to produce a transcript labeled like a script, e.g., “Speaker 1: [utterance] … Speaker 2: [response] …”, with each change of speaker on a new line. 

If the user provided a list of known names (perhaps via a config file or arguments like `--names Alice,Bob,GM`), the tool will assign these to the detected speakers (matching them either to known voice signatures if a ML model is used, or simply in order or by clues in the content). If no names are given, generic labels (“Speaker 1”, “Speaker 2”, etc.) are used consistently throughout. The output of this step is an **annotated transcript**. Internally, the tool will use this annotated version for summarization input (so the LLM knows which character did each action or said each line, which is crucial for context). If the user requested an output file for the diarized transcript, the tool writes it out at this point (and logs “Diarized transcript saved to XYZ”). The diarization process might also output some stats or warnings (for example, if it believes there were 4 speakers but only 3 names were provided, it might warn and use a generic label for the extra speaker). 

*(If the input was a text transcript instead of audio, this diarization step is skipped **unless** we detect that the transcript has no speaker labels and the user specifically wants them. In the common case, an existing transcript might already have speakers labeled or might be more of a summary. We assume an existing transcript is already in a usable form, so we proceed directly to summarization with the given text. The user may choose to format their transcript with speaker labels beforehand if they want the LLM to know who is who. The tool will not attempt diarization on a provided text transcript unless explicitly instructed, since that would involve the same kind of inference as above but was not requested.)*

### 4. Summarization  
With a complete transcript in hand (either freshly transcribed from audio, or the user-provided text), the application next generates the summary. It sends the transcript (or a relevant portion of it, if the transcript is extremely large and needs to be truncated or summarized in parts) to an OpenAI large language model (LLM), along with a carefully crafted prompt. The prompt will instruct the LLM to produce a detailed summary of the RPG session, emphasizing all plot points, character actions, and decisions made. It might say, for example: *“Summarize the following RPG session transcript in detail. Focus on the narrative of events, the actions and decisions of each player character, outcomes of those actions, and important dialogue. Omit trivial banter and summarize long conversations. Include quotes only if they are important or memorable. The final output should read like session notes or a story recap, in Markdown format.”* The entire diarized transcript (or its segments) is then provided to the model as context. 

The summarization is done in one or multiple calls to the LLM, depending on length. For a moderate-length transcript that fits within the model’s context window, a single call may suffice. For very long sessions (e.g. several hours of gameplay resulting in tens of thousands of words of transcript), the tool might break the transcript into chronological chunks, summarize each chunk, and then possibly ask the LLM to combine those summaries into a coherent whole. The user is kept informed via progress messages, such as “Summarizing session (this may take a moment)…”. If chunking is required, a progress bar might indicate progress through the chunks. The LLM’s output is captured by the tool as the session **summary text**. 

The content of the summary is then lightly post-processed: the tool ensures it’s formatted in Markdown (e.g., adding a title or session name at the top, ensuring proper use of headings for different parts of the session like “### Scene 1: Confrontation at the Tavern”, etc., if applicable). It also checks that no obviously hallucinated information is included (the prompt will have encouraged faithful summarization; since the transcript is provided, the model should not fabricate unrelated content). The summary is expected to be **detailed** – it will likely be several paragraphs long, covering the beginning, middle, and end of the session’s events. All major decisions (for example, *“the party decided to trust the old man and follow him into the forest”*) and their consequences, important combat outcomes, puzzle solutions, new plot hooks, and character developments are included. Minor details (like every dice roll or every slight side conversation) are omitted for brevity, unless they were consequential or funny enough to note. The result should feel like a well-written recap or after-action report of the game session.

### 5. Writing Outputs and Completion  
Finally, the tool writes the summary and any other requested outputs to files. The **markdown summary** is written to the path provided by the user. The diarized full transcript (if requested) was likely already written earlier, but if not, it is saved now as well. The tool then prints a success message indicating that the operation is complete and where the outputs are. For example: “✅ Summary of the session has been generated and saved to `./Session1_Summary.md`”. If the diarized transcript was saved, it might say: “💬 Full transcript with speaker labels saved to `./Session1_Transcript.txt`”. These messages mark the end of execution. The program then exits cleanly. 

Throughout the run, the console output (progress bars, status messages, etc.) has kept the user informed, so by the end there are no surprises. In summary, if the input was valid and no unexpected errors occurred, the user will have their markdown summary file ready to open and read, and optionally a transcript file. The entire process is batch-oriented (not interactive), and typically completes in a timespan proportional to the audio length (transcription is the longest part, often taking real-time or slower) plus some seconds for summarization. A 3-hour session might take several minutes to transcribe and a minute or two to summarize with GPT-4, for example. The design ensures the user can simply start the tool and come back when it’s done, confident that they’ll either get results or a clear explanation of an issue.

## Technical Considerations  

**Environment & Dependencies:** **summarize_rpg_session** is implemented in Python and intended for Unix-based systems. It will rely on certain external libraries and services: for transcription and summarization it uses OpenAI’s APIs (thus requiring network access and a valid API key), and for progress display it uses a console UI library (`tqdm` for simple progress bars or `rich` for more advanced output). The tool should be developed and tested on Python 3.x and Unix environments to ensure compatibility. It’s expected to run in a terminal (TTY) that supports carriage return updates (for progress bars) and UTF-8 output (for any special characters in transcripts or output). No special hardware is required, but transcription of long audio can be memory and CPU intensive if done locally; since we offload transcription to OpenAI’s cloud, the local resource load is minimal apart from handling large text in memory. Users must have their OpenAI API credentials configured (e.g. via an environment variable or config file as per OpenAI’s Python SDK requirements) – the tool will document this prerequisite but it’s not an interactive part of running the command. 

**OpenAI API Rate Limiting and Errors:** Using OpenAI’s services means we must handle their rate limits and occasional errors. The tool is designed to handle transient issues gracefully. If the transcription API returns a rate limit error, the tool will catch the exception and automatically retry after a delay. Specifically, it can implement exponential backoff (wait a short random time, retry, and increase wait if it fails again) to eventually get a resul ([Rate limits - OpenAI API](https://platform.openai.com/docs/guides/rate-limits#:~:text=One%20easy%20way%20to%20avoid,exponential%20backoff%20means%20performing))】. Similar handling will be in place for the summarization API calls. In case an irrecoverable error occurs (e.g. invalid API key or the service is down), the tool will output an error to the user indicating that the OpenAI service failed and suggest checking the API key or trying later. All interactions with external APIs will be wrapped in error handlers to ensure the program doesn’t crash due to an unhandled exception. Additionally, the size of the transcript might hit model limits – if so, the tool might need to automatically switch to a model with a larger context (if available, e.g. GPT-4-32k) or implement the chunking strategy described earlier. These details will be abstracted from the user, but logged for transparency (for example, “Note: Transcript was very long, used segmented summarization.”).

**Performance Considerations:** Transcribing long audio (several hours) and summarizing large transcripts are time-consuming tasks. The tool is not real-time; it’s meant for post-session processing. We assume the user expects to wait minutes (or tens of minutes for very long sessions) for results. To improve perceived performance, we use the progress bars and possibly print intermediate info (like “Transcription roughly 50% complete.”). If needed, we might allow the user to specify a lower transcription quality or to limit summary detail to speed up processing, but by default we aim for completeness over speed. Memory-wise, loading a long transcript (which could be hundreds of kilobytes of text) and sending it to the LLM might require chunking to avoid running out of memory or hitting API payload limits. The tool should monitor the length of text being sent to the LLM and break it up if necessary (ensuring to maintain coherence by overlapping content or summarizing iteratively). These are internal technical strategies and will be implemented to ensure the tool can handle large inputs gracefully.  

**Speaker Diarization Approach:** Speaker diarization in audio can be complex. The tool will favor simple, reliable methods to implement it. One approach is using a pre-trained model like **pyannote.audio** for speaker diarization, which can take the audio (or the raw transcript with timings) and output speaker segments. However, integrating such a model increases complexity and runtime. Alternatively, as noted, an AI-based inference using GPT on the transcript may be used: essentially asking the model to insert speaker labels based on contex ([Best solution for Whisper diarization/speaker labeling? - API - OpenAI Developer Community](https://community.openai.com/t/best-solution-for-whisper-diarization-speaker-labeling/505922#:~:text=However%2C%20what%20I%20have%20noticed,with%20a%20prompt%20like%20this))】. While not perfectly accurate, community experiences have found it “unusually good” if guided properl ([Best solution for Whisper diarization/speaker labeling? - API - OpenAI Developer Community](https://community.openai.com/t/best-solution-for-whisper-diarization-speaker-labeling/505922#:~:text=However%2C%20what%20I%20have%20noticed,with%20a%20prompt%20like%20this))】, especially when actual speaker names are provided as a referenc ([Best solution for Whisper diarization/speaker labeling? - API - OpenAI Developer Community](https://community.openai.com/t/best-solution-for-whisper-diarization-speaker-labeling/505922#:~:text=It%20works%20even%20better%20if,to%20get%20a%20foothold%20on))】. The product will choose an approach that balances accuracy and simplicity. Known limitation: diarization might confuse speakers if their voices are similar or if everyone speaks in the same style. The tool will do its best, but it may sometimes misattribute a line. We mitigate this by allowing user-provided names (so at least labels remain consistent) and by focusing the summary more on *what* happened rather than *exactly who said each line* in cases of uncertainty. In any case, the diarized transcript is there for transparency, and the user can correct any mislabeling after the fact if needed. The product requirement is that some form of speaker labeling is present in the output; the exact implementation may evolve as better diarization tech becomes available. 

**Markdown Formatting and Output Quality:** The summary is in Markdown, which is a deliberate choice to enhance readability and portability. The tool will ensure that the Markdown syntax it uses is simple and widely compatible (headers, lists, maybe bold/italic, but nothing too platform-specific). This allows the summary to be easily viewed in a text editor or rendered on platforms like GitHub or Notion. We consider the possibility of embedding media or links in the summary if relevant (for instance, if the transcript or context provided an image or reference, the summary could link to it), but by default it’s text-only. The length of the summary will scale with the length of the session; it’s meant to be detailed, so a longer session yields a longer summary. This is a conscious decision – we prioritize completeness (so no important plot detail is left out) over brevity. Users who want a shorter summary can manually prune or could potentially run the summary through another pass with a “summarize the summary” prompt if they desire a quick synopsis. The PRD emphasizes that the summary must capture **all** essential points, so the development will focus on prompt engineering to make the LLM output as comprehensive as possible within one session’s scope.

**Security and Privacy:** Since audio and transcripts of RPG sessions might be private or sensitive (though typically less sensitive than business meetings, they are still user data), the tool should handle data responsibly. Audio files and transcripts are processed locally and sent to OpenAI’s API; users should be informed that their session content will be sent to a third-party (OpenAI) for processing. We assume users consent to this by using the tool (this could be mentioned in documentation). The tool itself does not store any data permanently aside from the output files. After running, any transcript in memory is cleared when the program exits. If any temporary files are made (e.g., if downloading an audio URL, a temp file may be saved), they are deleted after use. We also ensure the file permissions for output files are such that only the user can read them (honoring the system’s umask and not, for example, making them world-readable by default). 

**No Persistent State:** As mentioned, each invocation stands alone. This means if the user runs the tool for Session 1 and then Session 2, the tool does not remember anything about Session 1. This simplifies implementation (no need for a database or config across runs) and avoids accidental bleed of information. It does mean if the user has some custom settings (like a list of speaker names or a particular style for summaries), they need to provide those each time (possibly through a config file or always via arguments). We might allow a simple config file in the user’s home directory for convenience (to store API keys or default names), but that’s not required by this PRD. The focus is on a straightforward CLI usage each time. 

**Future Considerations:** While not strictly part of the requirements, it’s worth noting areas for future improvement. For example, supporting Windows out-of-the-box, adding more audio formats, or integrating with a scheduling system to auto-summarize after each game session are potential enhancements. Another idea is interactive use – perhaps integrating with a live Discord or Zoom session to transcribe in real-time – but that’s outside the current scope, which is offline, post-session processing. For now, **summarize_rpg_session** will fulfill the immediate need: given a recorded or transcribed RPG session, produce a clean, labeled transcript and a detailed markdown summary, with a smooth user experience on the command line. The design decisions above ensure the tool is reliable, user-friendly, and produces high-quality summaries that meaningfully capture the spirit and details of the players’ adventures.